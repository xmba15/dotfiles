# -*- mode: snippet -*-
# name: base_config
# key: base_config
# --
seed: 2024

num_workers: 4
experiment_name: ""

dataset:
  n_splits:
  fold_th:
  root_dir:
  train_dir: train
  val_dir: val
  train_val_dir: train_val
  test_dir: test

model:
  pretrained_model_name:
  num_classes:
  num_channels: 4
  ignore: 255
  pl_class:

optimizer:
  type: timm.optim.AdamW
  lr: 0.0001
  weight_decay: 0.001

scheduler:
  type: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: 100
  eta_min: 0.00001

trainer:
  devices: [0]
  accelerator: "cuda"
  max_epochs: 100
  gradient_clip_val: 5.0
  accumulate_grad_batches: 4
  log_every_n_steps: 50
  resume_from_checkpoint:

train_parameters:
  batch_size: 5

val_parameters:
  batch_size: 5

output_root_dir: experiments
img_h: 640
img_w: 864
